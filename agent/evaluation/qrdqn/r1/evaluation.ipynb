{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17803d7f-fd36-48d2-bf4e-6f22c4849b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from racesim.racesimulation import RaceSimulation\n",
    "from racesim.config import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baedac9-bf80-4259-a454-e771c443ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib.qrdqn import QRDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24fb0843-5c9b-47b6-9a24-9638c3298d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium_env.envs.f1_env import RewardFunctionPerPositionAtFinalLap\n",
    "from gymnasium_env.envs.f1_env import RewardFunctionPositionChangeByLapsToGo\n",
    "from gymnasium_env.envs.f1_env import RewardFunctionPositionChange\n",
    "from gymnasium_env.envs.f1_env import RewardFunctionPerPositionAtFinalLapEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede91694-d197-4369-ace5-7adacfc0ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 1000\n",
    "n_races_to_test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746cc76a-14b3-4a44-8bc1-d146970568a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantileNetwork(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (quantile_net): Sequential(\n",
       "    (0): Linear(in_features=220, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=84, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QRDQN.load(\"run_1/best_model.zip\")\n",
    "agent.quantile_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfb223-5d0e-4551-a491-7480158b20c1",
   "metadata": {},
   "source": [
    "## QR-DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9cf6e-41f3-4b12-849e-43e5f3f31eef",
   "metadata": {},
   "source": [
    "### R1 - Run 1 Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd06c7e4-d362-4955-adff-809f4abcdf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [09:25<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.283\n",
      "6.533369039018078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_1/best_model.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda6946-1378-4e89-9ca4-000434a320d7",
   "metadata": {},
   "source": [
    "### R1 - Run 1 Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53fd2336-3ebe-4262-8687-fb651ebb47d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [09:32<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.779\n",
      "5.0900057956745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_1/model_reward_position_change_0.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc666aa-3f85-402c-b824-12c8dc27fb2e",
   "metadata": {},
   "source": [
    "### R1 - Run 2 Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36894934-b487-4f4e-b2c7-3bbaab6e4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [09:28<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.511\n",
      "5.5300885164706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_2/best_model.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d81cb-5a4a-44b1-adca-699283b5d824",
   "metadata": {},
   "source": [
    "### R1 - Run 2 Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30d1e48-adff-4a3a-8b97-9863e4d6c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [09:37<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.344\n",
      "5.0045643167013045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_2/model_reward_position_change_1.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff43da-d941-4b4a-8a64-036e598f096f",
   "metadata": {},
   "source": [
    "### R1 - Run 3 Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6aca76-7f53-4d76-ac11-8aa53c8adb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [09:39<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.974\n",
      "6.070529136739235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_3/best_model.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c68061-d963-4160-bfff-9836851b7dcc",
   "metadata": {},
   "source": [
    "### R1 - Run 3 Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb8ea00-89db-4878-b158-83e17884199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [10:19<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.459\n",
      "6.313661299119554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qr_dqn_r1_start = []\n",
    "qr_dqn_r1_finish = []\n",
    "agent = QRDQN.load(\"run_3/model_reward_position_change_2.zip\")\n",
    "\n",
    "for seed in tqdm(range(n_seeds)):\n",
    "    env_basic = gym.make('F1Env/Basic-v0',seed=seed, reward_function = RewardFunctionPerPositionAtFinalLapEval(),\n",
    "                     grid_config=GridConfigMixed(), control_driver=True)\n",
    "    obs, info = env_basic.reset()\n",
    "    qr_dqn_r1_start.append(env_basic.our_driver.position)\n",
    "    # print(env_basic.our_driver.tyre.__dict__, \" - Start Position: \", env_basic.our_driver.position)\n",
    "    done = False\n",
    "    total_rw = 0\n",
    "    stops = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        if action != 0:\n",
    "            stops += 1\n",
    "        next_obs, reward, terminated, truncated, info = env_basic.step(action)\n",
    "\n",
    "        total_rw += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    qr_dqn_r1_finish.append(env_basic.our_driver.position)\n",
    "    # print(\"STOPS: \", stops)\n",
    "\n",
    "# print(a2c_r1_start\n",
    "# print(a2c_r1_finish)\n",
    "print(np.array(qr_dqn_r1_finish).mean())\n",
    "print(np.array(qr_dqn_r1_finish).std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
