{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec92442-8fa4-47a3-ba6d-1b0bd2ba8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, sync_envs_normalization\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923d84cc-7d58-43bc-a907-0ac8d977814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium_env.envs.f1_env import RewardFunction\n",
    "from gymnasium_env.envs.f1_env import RewardFunctionPerPositionAtFinalLap\n",
    "\n",
    "from racesim.config import GridConfigSortedById\n",
    "from racesim.config import GridConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a941d97-06fa-4b1f-870a-c407bbe027c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1EnvEvalCallback(EvalCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: Union[gym.Env, VecEnv],\n",
    "        callback_on_new_best: Optional[BaseCallback] = None,\n",
    "        callback_after_eval: Optional[BaseCallback] = None,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        log_path: Optional[str] = None,\n",
    "        best_model_save_path: Optional[str] = None,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        verbose: int = 1,\n",
    "        warn: bool = True,\n",
    "        seed_eval_env: int = 0,\n",
    "        reward_function: RewardFunction = None,\n",
    "        grid_config: GridConfig = None\n",
    "    ):\n",
    "        super().__init__(eval_env,callback_on_new_best,callback_after_eval,n_eval_episodes,eval_freq,\n",
    "                         log_path,best_model_save_path,deterministic,render,verbose,warn)\n",
    "        self.seed_eval_env = seed_eval_env\n",
    "        self.reward_function = reward_function\n",
    "        self.grid_config = grid_config\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = super()._on_step()\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            self.set_env_for_next_eval()\n",
    "        return continue_training\n",
    "        \n",
    "\n",
    "    def set_env_for_next_eval(self) -> None:\n",
    "        self.eval_env = gym.make(\"F1Env/Basic-v0\", seed=self.seed_eval_env,\n",
    "                                 reward_function=self.reward_function, grid_config=self.grid_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bf67cb-b37d-4681-850e-43b8d8f79a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "n_seeds = 3\n",
    "seed_eval = 50\n",
    "reward_function = RewardFunctionPerPositionAtFinalLap()\n",
    "\n",
    "grid_config = GridConfigSortedById()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bb8f6d-6baf-40f0-aa04-dcf7bde082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env_kwards = {\"seed\": seed_eval, \"reward_function\": reward_function, \"grid_config\": grid_config, \"render_mode\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9692a907-40b9-4b79-bc98-1c57b6e435f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "\n",
      " RUN 0\n",
      "\n",
      "--------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/miniconda3/envs/rfl/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-0.34 +/- 2.12\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=375000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=0.90 +/- 2.84\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=425000, episode_reward=2.23 +/- 3.02\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=3.50 +/- 3.64\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=475000, episode_reward=3.33 +/- 3.38\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=0.61 +/- 2.39\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=2.44 +/- 2.92\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=3.44 +/- 4.07\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=3.98 +/- 3.15\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=2.84 +/- 3.38\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=2.96 +/- 3.64\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=2.66 +/- 3.35\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=2.52 +/- 3.86\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=3.71 +/- 4.52\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=3.11 +/- 3.30\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=2.54 +/- 3.36\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=2.88 +/- 3.34\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=3.38 +/- 3.20\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=4.51 +/- 4.44\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=850000, episode_reward=3.33 +/- 3.88\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=4.25 +/- 2.71\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=3.74 +/- 3.10\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=4.72 +/- 2.74\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=950000, episode_reward=2.79 +/- 3.36\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=3.81 +/- 3.19\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=4.14 +/- 3.42\n",
      "Episode length: 77.00 +/- 0.00\n",
      "-----------------\n",
      "\n",
      " RUN 1\n",
      "\n",
      "--------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-0.40 +/- 1.86\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=0.03 +/- 2.24\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=1.24 +/- 2.73\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=2.98 +/- 2.87\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=1.46 +/- 2.83\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-0.06 +/- 1.60\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-0.55 +/- 1.35\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=0.75 +/- 2.24\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=0.50 +/- 2.51\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=2.60 +/- 3.15\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=2.74 +/- 3.40\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.90 +/- 2.96\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=2.94 +/- 3.07\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=2.96 +/- 3.68\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=3.00 +/- 3.48\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=625000, episode_reward=2.02 +/- 3.36\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1.27 +/- 2.67\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=3.27 +/- 3.66\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=2.25 +/- 3.19\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=3.26 +/- 3.13\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=3.58 +/- 2.95\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=775000, episode_reward=3.10 +/- 3.05\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=4.84 +/- 3.35\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=825000, episode_reward=4.76 +/- 3.63\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=4.29 +/- 2.92\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1.10 +/- 1.93\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=3.08 +/- 1.68\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=2.84 +/- 2.95\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=4.70 +/- 4.05\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=2.11 +/- 2.53\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=3.94 +/- 2.82\n",
      "Episode length: 77.00 +/- 0.00\n",
      "-----------------\n",
      "\n",
      " RUN 2\n",
      "\n",
      "--------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-0.57 +/- 1.28\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=-0.57 +/- 1.28\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=2.70 +/- 2.72\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=1.56 +/- 3.73\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=3.55 +/- 3.12\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500000, episode_reward=1.49 +/- 2.23\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=4.26 +/- 3.77\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=550000, episode_reward=0.72 +/- 1.93\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=2.58 +/- 2.69\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=2.24 +/- 3.17\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=2.01 +/- 2.56\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=2.42 +/- 3.33\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=3.20 +/- 3.23\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2.38 +/- 3.06\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=2.30 +/- 3.09\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=3.36 +/- 3.60\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=2.67 +/- 2.97\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=2.74 +/- 3.30\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=2.74 +/- 2.67\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=2.58 +/- 3.05\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=0.96 +/- 3.33\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=2.51 +/- 3.03\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=4.75 +/- 3.72\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=950000, episode_reward=4.56 +/- 3.81\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=5.54 +/- 3.15\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=2.39 +/- 2.36\n",
      "Episode length: 77.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_seeds):\n",
    "    print(\"-----------------\\n\\n\" + \" RUN \" + str(i) + \"\\n\\n--------------------------\")\n",
    "\n",
    "    env_kwargs = {\"seed\": i, \"reward_function\": reward_function, \"grid_config\": grid_config, \"render_mode\": None}\n",
    "    env = make_vec_env(\"F1Env/Basic-v0\", n_envs=2, vec_env_cls=SubprocVecEnv, env_kwargs=env_kwargs)\n",
    "    env_to_eval = make_vec_env(\"F1Env/Basic-v0\", n_envs=1, vec_env_cls=SubprocVecEnv, env_kwargs=eval_env_kwards)\n",
    "\n",
    "    eval_callback = F1EnvEvalCallback(eval_env=env_to_eval, best_model_save_path=\"./logs_\" + str(i) + \"/\",\n",
    "                             log_path=\"./logs_\" + str(i) + \"/\", n_eval_episodes=20, eval_freq=12500,\n",
    "                             deterministic=True, render=False, seed_eval_env=seed_eval,\n",
    "                                      reward_function=reward_function, grid_config=grid_config)\n",
    "\n",
    "    model_reward_position_change = A2C(\"MlpPolicy\", env, gamma=1, n_steps=64, learning_rate=0.00035,\n",
    "                                       verbose=0, tensorboard_log=\"./a2c_r3/\", seed=i)\n",
    "    model_reward_position_change.learn(total_timesteps=1e6, callback=eval_callback)\n",
    "    model_reward_position_change.save(path=\"./model_reward_position_change_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915a48b1-c793-4a75-8f86-baf47f9b62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from stable_baselines3.common.utils import obs_as_tensor\n",
    "# env_to_test = gym.make(\"F1Env/Basic-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216ab57a-cbd5-4dc0-ac12-1f43e82e2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = model_reward_position_change\n",
    "# stops = 0\n",
    "\n",
    "# for episode in tqdm(range(1)):\n",
    "#     t = 0\n",
    "#     total_rw = 0\n",
    "#     obs, info = env_to_test.reset()\n",
    "#     print(env_to_test.our_driver.tyre.__dict__)\n",
    "#     # print(obs)\n",
    "#     # print(\"------\")\n",
    "#     print(\"START POSITION: \", env_to_test.our_driver.position)\n",
    "#     done = False\n",
    "\n",
    "#     # play one episode\n",
    "#     while not done:\n",
    "#         # display(env_to_test.race_simulation.to_df())\n",
    "#         action, _ = agent.predict(obs, deterministic=True)\n",
    "#         with torch.no_grad():\n",
    "#             observation = obs.reshape((-1,) + agent.observation_space.shape)\n",
    "#             observation = obs_as_tensor(observation, 'cpu')\n",
    "#             q_values = agent.q_net(observation)\n",
    "#             # print(q_values)\n",
    "#         if action != 0:\n",
    "#             stops+=1\n",
    "#             print(action, \" - laps to go: \", env_to_test.our_driver.laps_to_go, \" - position: \", env_to_test.our_driver.position)\n",
    "#             print(q_values)\n",
    "#         next_obs, reward, terminated, truncated, info = env_to_test.step(action)\n",
    "#         # print('REWARDA: ', reward)\n",
    "\n",
    "#         total_rw += reward\n",
    "\n",
    "#         done = terminated or truncated\n",
    "#         obs = next_obs\n",
    "#         # print(obs)\n",
    "#         # print(\"------\")\n",
    "#         t+=1\n",
    "\n",
    "#     # print(\"Reward: \" , total_rw)\n",
    "# # display(env_to_test.race_simulation.to_df())\n",
    "# print(\"Reward: \" , total_rw, \" - \", \"Stops: \", stops)\n",
    "# env_to_test.race_simulation.driver_being_controlled.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf26b1b-2bda-4859-bc8c-7ed601f9ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_to_test.race_simulation.to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
