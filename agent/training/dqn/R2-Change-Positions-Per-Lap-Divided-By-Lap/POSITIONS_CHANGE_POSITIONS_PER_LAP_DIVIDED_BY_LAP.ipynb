{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec92442-8fa4-47a3-ba6d-1b0bd2ba8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "from stable_baselines3 import PPO\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, sync_envs_normalization\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923d84cc-7d58-43bc-a907-0ac8d977814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium_env.envs.f1_env import RewardFunction\n",
    "from gymnasium_env.envs.f1_env import RewardFunctionPositionChangeByLapsToGo\n",
    "\n",
    "from racesim.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a941d97-06fa-4b1f-870a-c407bbe027c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1EnvEvalCallback(EvalCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: Union[gym.Env, VecEnv],\n",
    "        callback_on_new_best: Optional[BaseCallback] = None,\n",
    "        callback_after_eval: Optional[BaseCallback] = None,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        log_path: Optional[str] = None,\n",
    "        best_model_save_path: Optional[str] = None,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        verbose: int = 1,\n",
    "        warn: bool = True,\n",
    "        seed_eval_env: int = 0,\n",
    "        reward_function: RewardFunction = None,\n",
    "        grid_config: GridConfig = None\n",
    "    ):\n",
    "        super().__init__(eval_env,callback_on_new_best,callback_after_eval,n_eval_episodes,eval_freq,\n",
    "                         log_path,best_model_save_path,deterministic,render,verbose,warn)\n",
    "        self.seed_eval_env = seed_eval_env\n",
    "        self.reward_function = reward_function\n",
    "        self.grid_config = grid_config\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = super()._on_step()\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            self.set_env_for_next_eval()\n",
    "        return continue_training\n",
    "        \n",
    "\n",
    "    def set_env_for_next_eval(self) -> None:\n",
    "        self.eval_env = gym.make(\"F1Env/Basic-v0\", seed=self.seed_eval_env,\n",
    "                                 reward_function=self.reward_function, grid_config=self.grid_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bf67cb-b37d-4681-850e-43b8d8f79a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "n_seeds = 3\n",
    "seed_eval = 50\n",
    "reward_function = RewardFunctionPositionChangeByLapsToGo()\n",
    "grid_config = GridConfigSortedById()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9692a907-40b9-4b79-bc98-1c57b6e435f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "\n",
      " RUN 0\n",
      "\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/miniconda3/envs/rfl/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=25000, episode_reward=-3.99 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.73 +/- 0.80\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-0.79 +/- 1.01\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1.22 +/- 1.31\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-3.99 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-3.30 +/- 1.65\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-3.34 +/- 2.14\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3.10 +/- 1.32\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-3.94 +/- 1.96\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-3.56 +/- 1.97\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-1.38 +/- 3.58\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-1.52 +/- 4.51\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-0.23 +/- 3.18\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=-0.78 +/- 3.14\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-1.38 +/- 2.68\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-0.54 +/- 3.63\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1.26 +/- 4.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=-2.38 +/- 6.08\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-1.95 +/- 5.12\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-1.73 +/- 5.84\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-0.54 +/- 6.17\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-2.60 +/- 5.09\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1.39 +/- 4.32\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=0.88 +/- 5.26\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=0.36 +/- 6.01\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1.38 +/- 4.63\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=2.62 +/- 4.76\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=1.98 +/- 4.94\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1.61 +/- 3.72\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=3.83 +/- 4.94\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=775000, episode_reward=0.46 +/- 4.30\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=3.28 +/- 4.36\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=3.09 +/- 3.19\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=4.55 +/- 3.16\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=875000, episode_reward=3.54 +/- 1.97\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1.84 +/- 3.06\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=3.84 +/- 2.75\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=4.32 +/- 2.48\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=5.02 +/- 2.85\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=2.82 +/- 2.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "-----------------\n",
      "\n",
      " RUN 1\n",
      "\n",
      "--------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-3.99 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.04 +/- 1.09\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-1.99 +/- 1.48\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2.51 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-3.58 +/- 2.09\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2.91 +/- 1.95\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-2.09 +/- 1.62\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-2.77 +/- 1.81\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-2.72 +/- 2.11\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-3.06 +/- 1.80\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-1.04 +/- 3.37\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-0.73 +/- 3.69\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=-2.04 +/- 3.02\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-1.34 +/- 4.05\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=0.22 +/- 4.31\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=1.26 +/- 4.76\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=425000, episode_reward=-0.06 +/- 4.83\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-0.36 +/- 5.67\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-2.64 +/- 5.69\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-1.76 +/- 6.04\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.25 +/- 6.17\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-3.08 +/- 6.04\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1.36 +/- 6.24\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=2.37 +/- 4.59\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=625000, episode_reward=4.04 +/- 2.78\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=3.37 +/- 3.70\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=2.08 +/- 4.16\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2.49 +/- 3.70\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=4.59 +/- 2.23\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=750000, episode_reward=4.69 +/- 2.81\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=775000, episode_reward=5.08 +/- 2.71\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=5.50 +/- 2.37\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=825000, episode_reward=4.06 +/- 3.69\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=3.30 +/- 3.04\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=3.18 +/- 2.55\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=4.20 +/- 2.99\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=3.98 +/- 3.40\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=3.26 +/- 3.19\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=3.70 +/- 2.89\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=4.53 +/- 2.26\n",
      "Episode length: 77.00 +/- 0.00\n",
      "-----------------\n",
      "\n",
      " RUN 2\n",
      "\n",
      "--------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-3.99 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.42 +/- 0.98\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-0.95 +/- 0.76\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-2.64 +/- 2.31\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-4.15 +/- 1.55\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-3.99 +/- 1.57\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-3.67 +/- 1.95\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3.12 +/- 1.93\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-2.53 +/- 1.66\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-0.51 +/- 3.92\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=0.93 +/- 3.22\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=-0.39 +/- 4.34\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-1.79 +/- 3.65\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-2.02 +/- 4.04\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-1.60 +/- 4.14\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-0.22 +/- 4.10\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-1.91 +/- 4.84\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-1.29 +/- 5.14\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-1.69 +/- 5.17\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-5.51 +/- 4.52\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-1.44 +/- 6.79\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=0.04 +/- 6.24\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-0.07 +/- 5.07\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=2.32 +/- 4.73\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=625000, episode_reward=1.00 +/- 5.39\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=2.26 +/- 3.84\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=3.59 +/- 4.48\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=2.34 +/- 4.01\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=2.38 +/- 4.11\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=4.90 +/- 2.86\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=775000, episode_reward=3.34 +/- 3.50\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=3.65 +/- 3.76\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=4.33 +/- 2.27\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=4.09 +/- 2.94\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=5.36 +/- 2.46\n",
      "Episode length: 77.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=900000, episode_reward=4.51 +/- 3.35\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=2.89 +/- 2.62\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=5.34 +/- 2.67\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=4.26 +/- 2.78\n",
      "Episode length: 77.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=4.76 +/- 2.21\n",
      "Episode length: 77.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_seeds):\n",
    "    print(\"-----------------\\n\\n\" + \" RUN \" + str(i) + \"\\n\\n--------------------------\")\n",
    "    env = gym.make(\"F1Env/Basic-v0\", seed=i, reward_function=reward_function, grid_config=grid_config)\n",
    "    env_to_eval = gym.make(\"F1Env/Basic-v0\", seed=seed_eval, reward_function=reward_function, grid_config=grid_config)\n",
    "\n",
    "    eval_callback = F1EnvEvalCallback(eval_env=env_to_eval, best_model_save_path=\"./logs_\" + str(i) + \"/\",\n",
    "                             log_path=\"./logs_\" + str(i) + \"/\", n_eval_episodes=20, eval_freq=25000,\n",
    "                             deterministic=True, render=False, seed_eval_env=seed_eval,\n",
    "                                      reward_function=reward_function, grid_config=grid_config)\n",
    "\n",
    "    model_reward_position_change = DQN(\"MlpPolicy\", env, gamma=1, buffer_size=100000, learning_starts=5000,\n",
    "                                            train_freq=(16,\"step\"), exploration_initial_eps=0.25, exploration_fraction=0.2,\n",
    "                                            exploration_final_eps=0.020, learning_rate=0.00005,\n",
    "                                            batch_size=256,\n",
    "                                            verbose=0, tensorboard_log=\"./dqn_r2/\", seed=i)\n",
    "    model_reward_position_change.learn(total_timesteps=1e6, callback=eval_callback)\n",
    "    model_reward_position_change.save(path=\"./model_reward_position_change_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0713466-6ab2-4a2d-b8a6-f4cacfc2a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from stable_baselines3.common.utils import obs_as_tensor\n",
    "# env_to_test = gym.make(\"F1Env/Basic-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12d0dbf-9955-43c0-bdc3-d989de881612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound': 0, 'base_lap_time': 70.0193, 'squared_deg': 0.00281, 'linear_deg': -0.01999, 'used_laps': 0}\n",
      "START POSITION:  14\n",
      "2  - laps to go:  71  - position:  13\n",
      "tensor([[ 0.0537, -0.4632,  0.0647, -0.4645]])\n",
      "1  - laps to go:  51  - position:  20\n",
      "tensor([[2.1372, 2.1472, 2.0500, 1.5315]])\n",
      "2  - laps to go:  40  - position:  14\n",
      "tensor([[-0.4095, -0.6065, -0.3871, -1.5063]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  4  -  Stops:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent = model_reward_position_change\n",
    "# stops = 0\n",
    "\n",
    "# for episode in tqdm(range(1)):\n",
    "#     t = 0\n",
    "#     total_rw = 0\n",
    "#     obs, info = env_to_test.reset()\n",
    "#     print(env_to_test.our_driver.tyre.__dict__)\n",
    "#     # print(obs)\n",
    "#     # print(\"------\")\n",
    "#     print(\"START POSITION: \", env_to_test.our_driver.position)\n",
    "#     done = False\n",
    "\n",
    "#     # play one episode\n",
    "#     while not done:\n",
    "#         # display(env_to_test.race_simulation.to_df())\n",
    "#         action, _ = agent.predict(obs, deterministic=True)\n",
    "#         with torch.no_grad():\n",
    "#             observation = obs.reshape((-1,) + agent.observation_space.shape)\n",
    "#             observation = obs_as_tensor(observation, 'cpu')\n",
    "#             q_values = agent.q_net(observation)\n",
    "#             # print(q_values)\n",
    "#         if action != 0:\n",
    "#             stops+=1\n",
    "#             print(action, \" - laps to go: \", env_to_test.our_driver.laps_to_go, \" - position: \", env_to_test.our_driver.position)\n",
    "#             print(q_values)\n",
    "#         next_obs, reward, terminated, truncated, info = env_to_test.step(action)\n",
    "#         # print('REWARDA: ', reward)\n",
    "\n",
    "#         total_rw += reward\n",
    "\n",
    "#         done = terminated or truncated\n",
    "#         obs = next_obs\n",
    "#         # print(obs)\n",
    "#         # print(\"------\")\n",
    "#         t+=1\n",
    "\n",
    "#     # print(\"Reward: \" , total_rw)\n",
    "# # display(env_to_test.race_simulation.to_df())\n",
    "# print(\"Reward: \" , total_rw, \" - \", \"Stops: \", stops)\n",
    "# env_to_test.race_simulation.driver_being_controlled.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f647bedb-5216-46f6-a1d5-f99e30c4370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>interval</th>\n",
       "      <th>gap</th>\n",
       "      <th>lapsToGo</th>\n",
       "      <th>lastLapTime</th>\n",
       "      <th>position</th>\n",
       "      <th>potentialLapTime</th>\n",
       "      <th>compound</th>\n",
       "      <th>usedLaps</th>\n",
       "      <th>secondDry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>73.217360</td>\n",
       "      <td>1</td>\n",
       "      <td>73.31914</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.402896</td>\n",
       "      <td>0.402896</td>\n",
       "      <td>0</td>\n",
       "      <td>73.423376</td>\n",
       "      <td>2</td>\n",
       "      <td>73.53250</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>35.195148</td>\n",
       "      <td>35.598044</td>\n",
       "      <td>0</td>\n",
       "      <td>73.742110</td>\n",
       "      <td>3</td>\n",
       "      <td>73.78440</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.255083</td>\n",
       "      <td>35.853127</td>\n",
       "      <td>0</td>\n",
       "      <td>74.018995</td>\n",
       "      <td>4</td>\n",
       "      <td>74.06536</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.877605</td>\n",
       "      <td>36.730733</td>\n",
       "      <td>0</td>\n",
       "      <td>74.038773</td>\n",
       "      <td>5</td>\n",
       "      <td>74.08638</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.203064</td>\n",
       "      <td>36.933797</td>\n",
       "      <td>0</td>\n",
       "      <td>73.946171</td>\n",
       "      <td>6</td>\n",
       "      <td>74.06914</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>0.267274</td>\n",
       "      <td>37.201070</td>\n",
       "      <td>0</td>\n",
       "      <td>73.976423</td>\n",
       "      <td>7</td>\n",
       "      <td>72.20032</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.046089</td>\n",
       "      <td>37.247159</td>\n",
       "      <td>0</td>\n",
       "      <td>73.661027</td>\n",
       "      <td>8</td>\n",
       "      <td>73.70020</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>5.169603</td>\n",
       "      <td>42.416762</td>\n",
       "      <td>0</td>\n",
       "      <td>73.521490</td>\n",
       "      <td>9</td>\n",
       "      <td>73.63220</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>6.542375</td>\n",
       "      <td>48.959137</td>\n",
       "      <td>0</td>\n",
       "      <td>72.836228</td>\n",
       "      <td>10</td>\n",
       "      <td>72.88560</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>11.694773</td>\n",
       "      <td>60.653910</td>\n",
       "      <td>0</td>\n",
       "      <td>73.601671</td>\n",
       "      <td>11</td>\n",
       "      <td>73.63980</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>60.695439</td>\n",
       "      <td>0</td>\n",
       "      <td>73.492078</td>\n",
       "      <td>12</td>\n",
       "      <td>73.60040</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.297962</td>\n",
       "      <td>60.993401</td>\n",
       "      <td>0</td>\n",
       "      <td>73.516239</td>\n",
       "      <td>13</td>\n",
       "      <td>72.32720</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>1.155284</td>\n",
       "      <td>62.148685</td>\n",
       "      <td>0</td>\n",
       "      <td>73.763442</td>\n",
       "      <td>14</td>\n",
       "      <td>73.80720</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>0.294137</td>\n",
       "      <td>62.442822</td>\n",
       "      <td>0</td>\n",
       "      <td>73.814550</td>\n",
       "      <td>15</td>\n",
       "      <td>73.52220</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>12.387470</td>\n",
       "      <td>74.830293</td>\n",
       "      <td>0</td>\n",
       "      <td>73.429952</td>\n",
       "      <td>16</td>\n",
       "      <td>73.54010</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.290057</td>\n",
       "      <td>75.120349</td>\n",
       "      <td>0</td>\n",
       "      <td>73.485042</td>\n",
       "      <td>17</td>\n",
       "      <td>70.55994</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>4.748642</td>\n",
       "      <td>79.868991</td>\n",
       "      <td>0</td>\n",
       "      <td>71.901510</td>\n",
       "      <td>18</td>\n",
       "      <td>70.47272</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>1.775355</td>\n",
       "      <td>81.644346</td>\n",
       "      <td>0</td>\n",
       "      <td>73.817668</td>\n",
       "      <td>19</td>\n",
       "      <td>73.86092</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>81.845112</td>\n",
       "      <td>0</td>\n",
       "      <td>70.412440</td>\n",
       "      <td>20</td>\n",
       "      <td>70.38950</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   interval        gap  lapsToGo  lastLapTime  position  \\\n",
       "0    6   0.000000   0.000000         0    73.217360         1   \n",
       "1   13   0.402896   0.402896         0    73.423376         2   \n",
       "2   14  35.195148  35.598044         0    73.742110         3   \n",
       "3    7   0.255083  35.853127         0    74.018995         4   \n",
       "4    1   0.877605  36.730733         0    74.038773         5   \n",
       "5    4   0.203064  36.933797         0    73.946171         6   \n",
       "6   16   0.267274  37.201070         0    73.976423         7   \n",
       "7   10   0.046089  37.247159         0    73.661027         8   \n",
       "8   15   5.169603  42.416762         0    73.521490         9   \n",
       "9   19   6.542375  48.959137         0    72.836228        10   \n",
       "10   5  11.694773  60.653910         0    73.601671        11   \n",
       "11   2   0.041528  60.695439         0    73.492078        12   \n",
       "12   3   0.297962  60.993401         0    73.516239        13   \n",
       "13  11   1.155284  62.148685         0    73.763442        14   \n",
       "14   8   0.294137  62.442822         0    73.814550        15   \n",
       "15  12  12.387470  74.830293         0    73.429952        16   \n",
       "16  18   0.290057  75.120349         0    73.485042        17   \n",
       "17   9   4.748642  79.868991         0    71.901510        18   \n",
       "18  17   1.775355  81.644346         0    73.817668        19   \n",
       "19   0   0.200766  81.845112         0    70.412440        20   \n",
       "\n",
       "    potentialLapTime  compound  usedLaps  secondDry  \n",
       "0           73.31914         0        38       True  \n",
       "1           73.53250         0        39       True  \n",
       "2           73.78440         2        45       True  \n",
       "3           74.06536         2        48       True  \n",
       "4           74.08638         2        48       True  \n",
       "5           74.06914         0        42       True  \n",
       "6           72.20032         0        32       True  \n",
       "7           73.70020         2        45       True  \n",
       "8           73.63220         0        40       True  \n",
       "9           72.88560         1        39       True  \n",
       "10          73.63980         2        44       True  \n",
       "11          73.60040         0        40       True  \n",
       "12          72.32720         0        33       True  \n",
       "13          73.80720         2        45       True  \n",
       "14          73.52220         2        44       True  \n",
       "15          73.54010         0        39       True  \n",
       "16          70.55994         0        18       True  \n",
       "17          70.47272         0        17       True  \n",
       "18          73.86092         2        46       True  \n",
       "19          70.38950         0        16       True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env_to_test.race_simulation.to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
